{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 26 19:51:26 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB           On | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   31C    P0               57W / 400W|      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB           On | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   30C    P0               57W / 400W|      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB           On | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   30C    P0               60W / 400W|      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB           On | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   30C    P0               55W / 400W|      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB           On | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   37C    P0               57W / 400W|      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB           On | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   36C    P0               58W / 400W|      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB           On | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   36C    P0               59W / 400W|      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB           On | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   35C    P0               59W / 400W|      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=2,3,4,5,6,7\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2,3,4,5,6,7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sja082/anaconda3/envs/cellstory/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers as tr\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the dataset\n",
    "\n",
    "We will use the [Rotten Tomatoes sentiment classification dataset](https://huggingface.co/datasets/rotten_tomatoes) for this session. It has binary labels for sentiment (positive or negative) and is a good dataset to demonstrate how to train a model on a small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 8530\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1066\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotten = load_dataset(\"rotten_tomatoes\")  # load the dataset\n",
    "rotten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8525</th>\n",
       "      <td>any enjoyment will be hinge from a personal th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8526</th>\n",
       "      <td>if legendary shlockmeister ed wood had ever ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8527</th>\n",
       "      <td>hardly a nuanced portrait of a young woman's b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8528</th>\n",
       "      <td>interminably bleak , to say nothing of boring .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8529</th>\n",
       "      <td>things really get weird , though not particula...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8530 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  label\n",
       "0     the rock is destined to be the 21st century's ...      1\n",
       "1     the gorgeously elaborate continuation of \" the...      1\n",
       "2                        effective but too-tepid biopic      1\n",
       "3     if you sometimes like to go to the movies to h...      1\n",
       "4     emerges as something rare , an issue movie tha...      1\n",
       "...                                                 ...    ...\n",
       "8525  any enjoyment will be hinge from a personal th...      0\n",
       "8526  if legendary shlockmeister ed wood had ever ma...      0\n",
       "8527  hardly a nuanced portrait of a young woman's b...      0\n",
       "8528    interminably bleak , to say nothing of boring .      0\n",
       "8529  things really get weird , though not particula...      0\n",
       "\n",
       "[8530 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotten['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the model and pre-process our dataset using its tokenizer\n",
    "\n",
    "We will be using the `t5-small` model, as it is small enough for this demo session. Larger models will take longer to train, and will also require more memory. As with usual deep learning tasks, we will use the same tokenizer used for pre-training to pre-process our dataset. `transformers` provides a `AutoTokenizer` class that will automatically select the correct tokenizer for the model we are using. For loading the model, `transformers` provides a `AutoModelForSeq2SeqLM` class that will automatically fetch the correct model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"t5-small\"\n",
    "\n",
    "# model = tr.AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)  # Alternative way to initialize the model\n",
    "model = tr.T5ForConditionalGeneration.from_pretrained(model_checkpoint)  # Initialize the model\n",
    "\n",
    "# tokenizer = tr.AutoTokenizer.from_pretrained(model_checkpoint)  # Alternative way to initialize the tokenizer\n",
    "tokenizer = tr.T5Tokenizer.from_pretrained(model_checkpoint)  # Initialize the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8530/8530 [00:01<00:00, 7348.41 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 7304.97 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 7258.78 examples/s]\n"
     ]
    }
   ],
   "source": [
    "classes = {0: \"negative\", 1: \"positive\"}  # Sentiment classes\n",
    "\n",
    "def map_fn(data):  # Function to tokenize the dataset\n",
    "    # Convert the dataset to a tokenized dataset, and remove the columns that are not needed anymore\n",
    "    return tokenizer(\n",
    "            data['text'],  # Tokenize the text\n",
    "            text_target=[classes[label] for label in data['label']],  # Convert 0/1 to \"negative\"/\"positive\" text labels\n",
    "            truncation=True,  # Truncate the inputs if they are too long\n",
    "            padding=True,  # Pad the inputs if they are too short\n",
    "            return_tensors='np'  # Return NumPy tensors\n",
    "        )\n",
    "\n",
    "# Tokenize the dataset\n",
    "rotten_tokenized = rotten.map(  # Maps the tokenize function to each split of the dataset\n",
    "    map_fn, \n",
    "    batched=True,  # Batch the outputs\n",
    "    remove_columns=['text', 'label']  # Remove the untokenized columns\n",
    ")  # Tokenize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup the training\n",
    "Hugging face has a [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) class that makes training models easy. It takes care of logging, checkpointing, and other bookkeeping tasks. It also has a `TrainingArguments` class that is used to configure the training job. \n",
    "\n",
    "By default, Hugging Face uses all the visible GPUs for training, which we set using the `CUDA_VISIBLE_DEVICES` environment variable. Earlier in this notebook, we set this to `1,2,3,4,5,6` to use all the GPUs on the system except for GPU 0 and 7. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params\n",
    "checkpoint_dir = \"checkpoints\"  # Directory to save the checkpoints to\n",
    "run_name = \"t5-small-rotten-tomatoes\"  # A name for the current training run\n",
    "epochs = 3  # Number of training epochs\n",
    "batch_size = 128  # Batch size\n",
    "optimizer = \"adamw_torch\"  # Optimizer to use. Adam with weight decay is a good standard choice\n",
    "# tensorboard_dir = \"tensorboard\"  # Directory to save TensorBoard logs to\n",
    "\n",
    "# Create the directories\n",
    "checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "# if not os.path.exists(tensorboard_dir):\n",
    "#     os.makedirs(tensorboard_dir)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = tr.TrainingArguments(\n",
    "    checkpoint_path,\n",
    "    num_train_epochs=epochs,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    optim=optimizer,\n",
    "    # fp16=True,  # Use FP16 training\n",
    "    # bf16=True,  # Use BF16 training (only on Ampere GPUs like the A100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the trainer\n",
    "data_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)  # Initialize the data collator (data collator does the same thing as a data loader in PyTorch)\n",
    "trainer = tr.Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=rotten_tokenized['train'],  # Pass in the tokenized training dataset\n",
    "    eval_dataset=rotten_tokenized['validation'],  # Pass in the tokenized validation dataset\n",
    "    data_collator=data_collator,  # Pass in the data collator\n",
    "    tokenizer=tokenizer,  # Pass in the tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tensorboard\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir tensorboard  # In VSCode, simply click on Launch TensorBoard and select the tensorboard directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sja082/anaconda3/envs/cellstory/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 00:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()  # Train the model\n",
    "\n",
    "trainer.save_model()  # Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 26 19:52:17 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB           On | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   31C    P0               56W / 400W|      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB           On | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   30C    P0               54W / 400W|      0MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB           On | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   33C    P0               61W / 400W|  10317MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB           On | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   32C    P0               56W / 400W|   7841MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB           On | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   39C    P0               58W / 400W|   7841MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB           On | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   38C    P0               59W / 400W|   7841MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB           On | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   39C    P0               61W / 400W|   7841MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB           On | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   38C    P0               61W / 400W|   7697MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    2   N/A  N/A    488619      C   ...anaconda3/envs/cellstory/bin/python    10314MiB |\n",
      "|    3   N/A  N/A    488619      C   ...anaconda3/envs/cellstory/bin/python     7838MiB |\n",
      "|    4   N/A  N/A    488619      C   ...anaconda3/envs/cellstory/bin/python     7838MiB |\n",
      "|    5   N/A  N/A    488619      C   ...anaconda3/envs/cellstory/bin/python     7838MiB |\n",
      "|    6   N/A  N/A    488619      C   ...anaconda3/envs/cellstory/bin/python     7838MiB |\n",
      "|    7   N/A  N/A    488619      C   ...anaconda3/envs/cellstory/bin/python     7694MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Predict using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = tr.AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)  # Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sja082/anaconda3/envs/cellstory/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Dieser Film ist eine Verschwendung von Zeit. Die Handlung ist schrecklich.',\n",
       " 'This movie is really good. The acting is great. The plot is interesting.',\n",
       " 'movie good but bad']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = [\n",
    "    \"\"\"\n",
    "This movie is a waste of time. The acting is terrible. The plot is ridiculous. I will never watch this movie again.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "This movie is really good. The acting is great. The plot is interesting. I will definitely watch this movie again.\n",
    "\"\"\",\n",
    "\"\"\"\n",
    "movie good but bad \n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "# Tokenize the reviews\n",
    "tokenized_reviews = tokenizer(\n",
    "    reviews,\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Generate the sentiment labels\n",
    "pred_labels = trained_model.generate(\n",
    "    input_ids=tokenized_reviews['input_ids'].to(trained_model.device),  # Convert the input to PyTorch tensors\n",
    "    attention_mask=tokenized_reviews['attention_mask'].to(trained_model.device),  # Convert the input to PyTorch tensors\n",
    ")\n",
    "\n",
    "# Decode the labels\n",
    "tokenizer.batch_decode(pred_labels, skip_special_tokens=True)  # Decode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. DeepSpeed accelerator\n",
    "\n",
    "We can further speed up training by using the [DeepSpeed](https://www.deepspeed.ai/) library. As models grow larger and larger, we need to optimally utilise every single computational resource available at our disposal. DeepSpeed provides a number of features that help us do this. We will be using the [ZeRO-Offload](https://www.deepspeed.ai/features/#zero-offload) feature of DeepSpeed to train our model. ZeRO-Offload allows us to train models that are larger than the GPU memory by offloading the optimizer states to the host memory. This allows us to train models that are much larger than the GPU memory.\n",
    "\n",
    "We can work with deepspeed using hugging face, by specifying a deepspeed configuration. There are a lot of options here that can be configured to obtain the best performance for your setup. You can read more about them [here](https://www.deepspeed.ai/docs/config-json/). We will be using the following configuration for this session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DeepSpeed zero optimization config\n",
    "zero_config = {\n",
    "    \"train_batch_size\": \"auto\",  # train_batch_size is the global batch size\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",  # train_micro_batch_size_per_gpu is the per-GPU batch size\n",
    "    # train_batch_size = train_micro_batch_size_per_gpu * gradient_accumulation_steps * number of GPUs\n",
    "\n",
    "    # ZeRO parameters\n",
    "    \"zero_optimization\": {\n",
    "        # \"stage\": 2,  # Enable ZeRO Stage 2\n",
    "        # \"offload_optimizer\": {\"device\": \"cpu\", \"pin_memory\": True},  # Offload the optimizer to the CPU  \n",
    "        \n",
    "        'stage': 3,\n",
    "        'stage3_gather_16bit_weights_on_model_save': True,  # Gather 16-bit weights to the full precision model during save (allows saving mixed precision models)\n",
    "\n",
    "        \"contiguous_gradients\": True,  # Enable contiguous gradients, which improves performance by reducing memory fragmentation\n",
    "        \"overlap_comm\": True,\n",
    "    },\n",
    "\n",
    "    # Optimizer\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\",\n",
    "            \"torch_adam\": True,\n",
    "        },\n",
    "    },\n",
    "\n",
    "    # Scheduler\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": \"auto\",\n",
    "            \"warmup_max_lr\": \"auto\",\n",
    "            \"warmup_num_steps\": \"auto\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description of some of the parameters in `zero_optimization`:\n",
    "- `stage`: Stage 0, 1, 2, and 3 refer to disabled, optimizer state partitioning, and optimizer+gradient state partitioning, and optimizer+gradient+parameter partitioning, respectively.\n",
    "- `offload_optimizer`: Whether to offload the optimizer state to the host memory (CPU/NVMe). This frees up GPU memory for larger models. You can also offload the model parameters to the host memory by using the `offload_param` parameter (only available in stage 3).\n",
    "- `overlap_comm`: Attempts to overlap the gradient reduction with the backward pass. This can help speed up training and reduce idle time.\n",
    "- `contiguous_gradients`: Whether to use contiguous memory for gradients. This can help speed up training by reducing the memory access time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train t5-small again, but this time using deepspeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"t5-small\"\n",
    "\n",
    "model = tr.AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_checkpoint,\n",
    ")\n",
    "\n",
    "# Define params\n",
    "checkpoint_dir = \"checkpoints\"  # Directory to save the checkpoints to\n",
    "run_name = \"t5-small-rotten-tomatoes-deepspeed\"  # A name for the current training run\n",
    "checkpoint_path = os.path.join(checkpoint_dir, run_name)\n",
    "\n",
    "# Define the training arguments\n",
    "training_args = tr.TrainingArguments(\n",
    "    checkpoint_path,\n",
    "    num_train_epochs=epochs,\n",
    "    \n",
    "    # per_device_train_batch_size=batch_size,\n",
    "    per_device_train_batch_size=batch_size * 2,  # Double the batch size\n",
    "\n",
    "    deepspeed=zero_config,  # Pass in the ZeRO config\n",
    "    # fp16=True,  # Use FP16 training\n",
    "    # bf16=True,  # Use BF16 training (only on Ampere GPUs like the A100)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the trainer\n",
    "data_collator = tr.DataCollatorWithPadding(tokenizer=tokenizer)  # Initialize the data collator (data collator does the same thing as a data loader in PyTorch)\n",
    "trainer = tr.Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=rotten_tokenized['train'],  # Pass in the tokenized training dataset\n",
    "    eval_dataset=rotten_tokenized['validation'],  # Pass in the tokenized validation dataset\n",
    "    data_collator=data_collator,  # Pass in the data collator\n",
    "    tokenizer=tokenizer,  # Pass in the tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only works with single GPUs! Use deepspeed launcher (CLI) for multi-GPU training\n",
    "# trainer.train()\n",
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To utilise deepspeed with more than one GPU, we need to use the `deepspeed` CLI utility, made available to use when we installed deepspeed. This utility takes care of setting up the environment variables and other things required to run deepspeed. Currently, the Jupyter notebook only supports single GPU training. I have put all the relevant code from this notebook into a separate python script, `deepspeed_demo.py`, which we will now run. I have also switched to using the `t5-base` model, which is a slightly larger model at 220 million parameters.\n",
    "\n",
    "We can use `deepspeed` to run our training script as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the command in terminal, after restarting this kernel to free up GPU memory\n",
    "# Change the number of GPUs to the number of GPUs you have\n",
    "# deepspeed --num_gpus=6 deepspeed_demo.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Hugging Face DeepSpeed Integration Docs](https://huggingface.co/docs/transformers/main_classes/deepspeed)\n",
    "- [LLMs with Hugging Face - DataBricks Academy | Kaggle](https://www.kaggle.com/code/aliabdin1/llm-04a-fine-tuning-llms)\n",
    "- [DeepSpeed ZeRO Tutorial](https://www.deepspeed.ai/tutorials/zero/)\n",
    "- [HuggingFace Transformers](https://huggingface.co/transformers/)\n",
    "- [HuggingFace Trainer](https://huggingface.co/transformers/main_classes/trainer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellstory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
