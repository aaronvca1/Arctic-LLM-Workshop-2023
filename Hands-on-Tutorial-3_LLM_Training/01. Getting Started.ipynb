{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face\n",
    "\n",
    "With the advent of transfer learning, it has become easier to train models for various different tasks. However, training a model from scratch is still a time consuming and resource intensive task. This is where Hugging Face comes in. Hugging Face is a company that provides a library of pre-trained models that can be used for various different tasks. These models can be used as is or fine-tuned for a specific task. Hugging Face also provides a library of datasets that can be used for training and evaluation of models. With the rise of LLMs and generative models, Hugging Face has become a popular choice for researchers and practitioners alike. Due to the ease of use and the large community, Hugging Face is an essential tool for anyone working with LLMs.\n",
    "\n",
    "The Hugging Face stack consists of multiple libraries. Some of these libraries that are relevant for LLMs are: `transformers`, `PEFT`, `datasets`, `accelerate`, etc. We will use some of these libraries for this hands-on session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using open datasets and pre-trained models\n",
    "\n",
    "Hugging Face has a host of models and datasets for various different tasks. To get started, you can head over to Hugging Face and check out the models and datasets available there for different tasks. <br /><br />\n",
    "<img src=\"assets/hf_tasks.png\" width=\"700\" />\n",
    "\n",
    "Clicking on a task takes you to a page with a list of models and datasets for that task. For example, the page for the task of text classification is shown below. You can also find helpful documentation, tutorials and videos on the same page. <br /><br />\n",
    "<img src=\"assets/hf_tasks_text_classif.png\" width=\"700\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading a dataset is easy. You can use the `datasets` library to load a dataset. The `datasets` library provides a unified API to load datasets from Hugging Face and other sources. You can also use the `datasets` library to load your own dataset. The `datasets` library also provides a host of useful features such as automatic caching, shuffling, batching, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset  # For loading datasets\n",
    "from transformers import pipeline  # For using a pretrained model and create an inference flow or a \n",
    "import textwrap  # For wrapping text nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xsum_dataset = load_dataset(  # Load the XSum dataset: A set of BBC articles and summaries.\n",
    "    \"xsum\", version=\"1.2.0\"  # You can use differnt datasets by changing the name and version (check the huggingface docs)\n",
    ")\n",
    "xsum_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full cost of damage in Newton Stewart, one of the areas worst affected, is\n",
      "still being assessed. Repair work is ongoing in Hawick and many roads in\n",
      "Peeblesshire remain badly affected by standing water. Trains on the west coast\n",
      "mainline face disruption due to damage at the Lamington Viaduct. Many businesses\n",
      "and householders were affected by flooding in Newton Stewart after the River\n",
      "Cree overfl...\n",
      "\n",
      "Clean-up operations are continuing across the Scottish Borders and Dumfries and\n",
      "Galloway after flooding caused by Storm Frank.\n"
     ]
    }
   ],
   "source": [
    "sample = xsum_dataset['train'][0]\n",
    "print(textwrap.fill(sample['document'][:400] + \"...\", width=80))  # Print the first 400 characters of the article.\n",
    "print()\n",
    "print(textwrap.fill(sample['summary'], width=80))  # Print the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and using a model for inference.\n",
    "\n",
    "We will utilise the t5-small model. A short description of the arguments used in the code below is given below:\n",
    "- `task`: The task for which the model was trained. A list of tasks can be found [here](https://huggingface.co/tasks).\n",
    "- `model`: The model to be used. In this case, it is t5-small, which is a good starter LLM with only 60 million parameters. A list of models available for summarization can be found [here](https://huggingface.co/models?pipeline_tag=summarization). You can also choose a different task from the sidebar and see the models available for it.\n",
    "- `min_length`, `max_length`: The minimum and maximum length of the output sequence in number of tokens. The output sequence will be truncated if it exceeds the maximum length. The output sequence will be padded if it is shorter than the minimum length. A token is, in general, a word or a punctuation mark. \n",
    "- `truncation`: Most LLMs have a maximum input length. If the input sequence is longer than the maximum length, it is truncated. Setting this argument to `True` ensures that the input sequence is truncated to the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A summarization pipeline\n",
    "summarizer = pipeline(\n",
    "    task=\"summarization\",  # The task we want to perform\n",
    "    model=\"t5-small\",  # The model checkpoint we want to use (t5-small is a small model, ~60M parameters)\n",
    "    min_length=30,  # The minimum length of the summary in # of tokens\n",
    "    max_length=100,  # The maximum length of the summary in # of tokens\n",
    "    truncation=True,  # Truncate the input sequences to max_length\n",
    ")\n",
    "\n",
    "# A translation pipeline\n",
    "translator = pipeline(\n",
    "    task='translation_en_to_de',   # Follows the format: translation_{source language}_to_{target language}\n",
    "    model='t5-small',\n",
    "    min_length=30,\n",
    "    max_length=100,\n",
    "    truncation=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the full cost of damage in Newton Stewart is still being assessed . many roads\n",
      "in peeblesshire remain badly affected by standing water . the water breached a\n",
      "retaining wall, flooding many commercial properties .\n",
      "\n",
      "Clean-up operations are continuing across the Scottish Borders and Dumfries and\n",
      "Galloway after flooding caused by Storm Frank.\n"
     ]
    }
   ],
   "source": [
    "# Summarize\n",
    "print(textwrap.fill(summarizer(sample['document'])[0]['summary_text'], width=80))   # Summarize the article using the summarizer pre-trained model.\n",
    "print()\n",
    "\n",
    "# Original summary\n",
    "print(textwrap.fill(sample['summary'], width=80))  # Print the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nach Überschwemmungen durch Sturm Frank laufen die Säuberungsmaßnahmen über die\n",
      "schottischen Grenzen und Dumfries und Galloway weiter.\n",
      "\n",
      "Clean-up operations are continuing across the Scottish Borders and Dumfries and\n",
      "Galloway after flooding caused by Storm Frank.\n"
     ]
    }
   ],
   "source": [
    "# Translate the summary to German\n",
    "print(textwrap.fill(translator(sample['summary'])[0]['translation_text'], width=80))\n",
    "print()\n",
    "# Original English summary\n",
    "print(textwrap.fill(sample['summary'], width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot classification\n",
    "Zero-shot learning entails using a model for a task it was not trained for. It typically is used for classification tasks with the categories being unknown during training. This ability seems to be an emergent feature of large language models(100M+, according to hugging face). Some of the models in this category achieve this by simply prompting the model to classify a given text into a list of categories, while others, more advanced ones, utilise Natural Language Inference (NLI) models to achieve this. \n",
    "\n",
    "NLI models are trained on sets of (premise, hypothesis) pairs, where the model is trained to predict the probability of the hypothesis being true given the premise. This has been observed to be producing good results for zero-shot classification (see for more details: [this blog post](https://joeddav.github.io/blog/2020/05/29/ZSL.html), [paper 1](https://arxiv.org/abs/2005.14165), [paper 2](https://arxiv.org/abs/1909.00161)). `facebook/bart-large-mnli` is one such NLI model available on hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 1.15k/1.15k [00:00<00:00, 9.03MB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 1.63G/1.63G [00:20<00:00, 80.2MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 296kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 899k/899k [00:00<00:00, 2.99MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 4.55MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 11.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(model=\"facebook/bart-large-mnli\", max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'This is a short demo session on zero-shot text classification with Hugging Face, conducted by Suyog.',\n",
       " 'labels': ['computer science',\n",
       "  'mathematics',\n",
       "  'sports',\n",
       "  'physics',\n",
       "  'chemistry',\n",
       "  'biology'],\n",
       " 'scores': [0.4396549165248871,\n",
       "  0.13937702775001526,\n",
       "  0.12538164854049683,\n",
       "  0.11936939507722855,\n",
       "  0.10196133702993393,\n",
       "  0.07425558567047119]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zero-shot classification example\n",
    "pipe(\n",
    "    \"This is a short demo session on zero-shot text classification with Hugging Face, conducted by Suyog.\",\n",
    "    candidate_labels=[\"computer science\", \"sports\", \"physics\", \"biology\", \"chemistry\", \"mathematics\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot classification\n",
    "Slightly different, but related to zero-shot learning, is few-shot learning. Few-shot learning entails using a model for a task it was not trained for, but with a few examples of the task being given to the model as a primer. It allows the models to perform well on tasks other than classification. You will typically find models optimized for few-shot learning in the `text-generation` category on hugging face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_pipeline = pipeline(task='text-generation', model=\"EleutherAI/gpt-neo-1.3B\", max_new_tokens=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this sentence, predict the object of the preposition:\n",
      "    [sentence]: I am giving a demo to people.\n",
      "    [object]: a video I am giving a demo to people.\n"
     ]
    }
   ],
   "source": [
    "# Wrong output when no samples are provided\n",
    "out = few_shot_pipeline(\"\"\"For this sentence, predict the object of the preposition:\n",
    "    [sentence]: I am giving a demo to people.\n",
    "    [object]:\"\"\"\n",
    ")\n",
    "print(textwrap.fill(out[0]['generated_text'], width=200, replace_whitespace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:35625 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For each sentence, predict the object of the preposition:\n",
      "\n",
      "    [sentence]: The quick brown fox jumped over the lazy dog.\n",
      "    [object]: dog\n",
      "    ****\n",
      "    [sentence]: The lazy dog jumped over the quick\n",
      "brown fox.\n",
      "    [object]: fox\n",
      "    ****\n",
      "    [sentence]: I like to eat bananas.\n",
      "    [object]: bananas\n",
      "    ****\n",
      "    [sentence]: I am giving a demo to people.\n",
      "    [object]: people\n",
      "    ****\n"
     ]
    }
   ],
   "source": [
    "# In this particular model, the primer examples need to be separated by a token specified by `eos_token_id`\n",
    "eos_token_id = few_shot_pipeline.tokenizer.encode(\"*****\")[0]\n",
    "\n",
    "# Few-shot example with sample outputs providedz\n",
    "out = few_shot_pipeline(\n",
    "    \"\"\"For each sentence, predict the object of the preposition:\n",
    "\n",
    "    [sentence]: The quick brown fox jumped over the lazy dog.\n",
    "    [object]: dog\n",
    "    ****\n",
    "    [sentence]: The lazy dog jumped over the quick brown fox.\n",
    "    [object]: fox\n",
    "    ****\n",
    "    [sentence]: I like to eat bananas.\n",
    "    [object]: bananas\n",
    "    ****\n",
    "    [sentence]: I am giving a demo to people.\n",
    "    [object]:\"\"\",\n",
    "    eos_token_id=eos_token_id,\n",
    ")\n",
    "\n",
    "print(textwrap.fill(out[0]['generated_text'], width=200, replace_whitespace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- [LLMs with Hugging Face - DataBricks Academy | Kaggle](https://www.kaggle.com/code/aliabdin1/llm-01-how-to-use-llms-with-hugging-face?scriptVersionId=140351055)\n",
    "- [HuggingFace Transformers](https://huggingface.co/transformers/)\n",
    "- [HuggingFace Datasets](https://huggingface.co/docs/datasets/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cellstory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
